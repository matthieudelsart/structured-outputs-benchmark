#!/usr/bin/env python
"""Utility to inspect the length (in tokens) of every prompt generated by
src.generate_vllm for the six benchmark tasks.

Usage
-----
uv run python -m src.check_prompt_lengths --model google/gemma-3-4b-it [--prompt-type low]

The script reproduces *exactly* the prompt-building logic from
*generate_vllm.py* (without the call to vLLM) so the numbers accurately
reflect what the GPU will see at inference time.
"""
from __future__ import annotations

import argparse
import json
import statistics
from pathlib import Path
from typing import Dict, List, Callable

try:
    from transformers import AutoTokenizer  # type: ignore
except ImportError:
    AutoTokenizer = None  # type: ignore

try:
    import tiktoken  # type: ignore
except ImportError:
    tiktoken = None  # type: ignore

BASE_DATA = Path("data/clean")

# ----------------------------------------------------------------------------
# Prompt helpers (mirrors generate_vllm.py)
# ----------------------------------------------------------------------------

def _load_prompt(template_path: Path) -> str:
    if not template_path.exists():
        raise FileNotFoundError(f"Prompt template not found: {template_path}")
    return template_path.read_text()


def _build_prompts_generic(data_path: Path, prompt_tpl: str) -> List[str]:
    bench = json.load((data_path / "bench.json").open())
    prompts: List[str] = []
    for rec in bench:
        prompt = (
            prompt_tpl.replace("{input}", rec["input"])
            if "{input}" in prompt_tpl
            else prompt_tpl + f"\n\nInput: {rec['input']}\nOutput:"
        )
        prompts.append(prompt)
    return prompts


def _build_prompts_wikibio(data_path: Path, prompt_tpl: str) -> List[str]:
    bench = json.load((data_path / "bench.json").open())
    prompts: List[str] = []
    for rec in bench:
        keys_fmt = ", ".join(f"'{k}'" for k in rec["keys"])
        prompt = prompt_tpl.replace("{{EXPECTED_KEYS}}", keys_fmt)
        prompt += f"\n\nInput: {rec['input']}\nOutput:"
        prompts.append(prompt)
    return prompts


def _build_prompts_apibank(data_path: Path, prompt_tpl: str) -> List[str]:
    """Replace only the {input} and {instruction} placeholders to avoid
    issues with other braces present in the template (e.g. JSON example)."""

    bench = json.load((data_path / "bench.json").open())

    def _fill(tpl: str, inp: str, instr: str) -> str:
        return tpl.replace("{input}", inp).replace("{instruction}", instr)

    return [
        _fill(prompt_tpl, rec["input"], rec["instruction"])
        for rec in bench
    ]

# ----------------------------------------------------------------------------


def _collect_prompts(prompt_type: str = "") -> Dict[str, List[str]]:
    """Return a mapping task_name -> list_of_prompts."""

    def tpl(task_dir: Path) -> str:
        return _load_prompt(task_dir / f"prompt{'_' + prompt_type if prompt_type else ''}.txt")

    tasks: Dict[str, List[str]] = {}

    # 1-Rotowire, 3-FewNERD, 4-TOPv1
    for t in ["1-rotowire", "3-few_nerd", "4-TOPv1"]:
        p = BASE_DATA / t
        tasks[t] = _build_prompts_generic(p, tpl(p))

    # 2-WikiBio
    p = BASE_DATA / "2-wiki_bio"
    tasks["2-wiki_bio"] = _build_prompts_wikibio(p, tpl(p))

    # 5-APIbank
    p = BASE_DATA / "5-api_bank"
    tasks["5-api_bank"] = _build_prompts_apibank(p, tpl(p))

    # 6-Reasoning subtasks
    for sub in ["GSM8K", "last_letter"]:
        p_sub = BASE_DATA / "6-reasoning" / sub
        prompts = _build_prompts_generic(p_sub, tpl(p_sub))
        tasks[f"6-reasoning/{sub}"] = prompts

    return tasks

# ----------------------------------------------------------------------------


def main() -> None:
    ap = argparse.ArgumentParser(description="Analyse prompt token lengths.")
    ap.add_argument(
        "--model",
        default=None,
        help=(
            "(Optional) HuggingFace model id or path. If provided, the exact tokenizer of that model is used. "
            "If omitted, the script defaults to the lightweight tiktoken 'cl100k_base' encoding, which is a good approximation."
        ),
    )
    ap.add_argument(
        "--prompt-type",
        choices=["", "low"],
        default="",
        help="Variant of the prompt templates to use (matches generate_vllm).",
    )
    args = ap.parse_args()

    # ---------------------------------------------------------------------
    if args.model is None:
        # Default: use tiktoken approximation -------------------------------------------------
        if tiktoken is None:
            raise RuntimeError(
                "tiktoken is not installed. Either install it (`pip install tiktoken`) or provide --model to use a HuggingFace tokenizer."
            )
        enc = tiktoken.get_encoding("cl100k_base")

        def _encode(text: str) -> List[int]:  # type: ignore
            return enc.encode(text)

    else:
        # Use HuggingFace tokenizer -----------------------------------------------------------
        if AutoTokenizer is None:
            raise RuntimeError(
                "transformers is not installed; install it (`pip install transformers`) or omit --model to fall back to tiktoken."
            )
        
        hf_tokenizer = AutoTokenizer.from_pretrained(args.model, trust_remote_code=True)

        def _encode(text: str) -> List[int]:  # type: ignore
            return hf_tokenizer.encode(text, add_special_tokens=False)

    # ---------------------------------------------------------------------

    all_prompts = _collect_prompts(args.prompt_type)

    print("Task\tCount\tMean\t95%\tMax (tokens)")
    print("──" * 24)

    global_max = 0
    for task, prompts in all_prompts.items():
        lengths = [len(_encode(p)) for p in prompts]
        mean = statistics.mean(lengths)
        p95 = statistics.quantiles(lengths, n=20)[18]  # 95th percentile
        m = max(lengths)
        global_max = max(global_max, m)
        print(f"{task}\t{len(prompts)}\t{mean:.1f}\t{p95}\t{m}")

    print("──" * 24)
    print(f"Overall max prompt length: {global_max} tokens")


if __name__ == "__main__":
    main() 